{"cells":[{"cell_type":"markdown","metadata":{"id":"KFgj2IhZoEJZ"},"source":["# 1. Анализ текстов с использованием метода Word2vec\n","\n","Word2vec - это еще один агоритм преобразования текстов в точки в векторном пространстве признаков. В его основе лежит нейронная сеть - автоэнкодер, который преобразует слово в вектор фиксированной длины. Особенность этого алгоритма заключается в том, что он учитывает семантику слов. Близкие по смыслу слова будут располагаться ближе друг к другу в векторном пространстве, а далекие - дальше. Если объем текстов достаточно большой, то с помощью модели word2vec мы можем определять синонимы и антонимы слов, и, благодаря этому, точность классификации текстов может увеличиться.\n","\n","Сегодня будем пользоваться теми же наборами текстов, что и в прошлый раз. Прежде всего установим уже известные вам библиотеки для работы с текстовыми данными.\n"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4027,"status":"ok","timestamp":1642957514455,"user":{"displayName":"Иван Романов","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"05119643280596747163"},"user_tz":-180},"id":"izBRyoiHyfxn","outputId":"d82d1fdc-c9a9-4e9d-d420-c1e48692de93"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[33mDEPRECATION: Python 2.7 reached the end of its life on January 1st, 2020. Please upgrade your Python as Python 2.7 is no longer maintained. pip 21.0 will drop support for Python 2.7 in January 2021. More details about Python 2 support in pip can be found at https://pip.pypa.io/en/latest/development/release-process/#python-2-support pip 21.0 will remove support for this functionality.\u001b[0m\n","Requirement already satisfied: pymorphy2 in /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages (0.9.1)\n","Collecting nltk\n","  Using cached nltk-3.5.zip (1.4 MB)\n","Requirement already satisfied: dawg-python>=0.7.1 in /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages (from pymorphy2) (0.7.2)\n","Requirement already satisfied: pymorphy2-dicts-ru<3.0,>=2.4 in /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages (from pymorphy2) (2.4.417127.4579844)\n","Requirement already satisfied: docopt>=0.6 in /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages (from pymorphy2) (0.6.2)\n","Requirement already satisfied: backports.functools_lru_cache>=1.0.1 in /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages (from pymorphy2) (1.6.1)\n","Requirement already satisfied: click in /Users/Int/Library/Python/2.7/lib/python/site-packages (from nltk) (7.1.2)\n","Requirement already satisfied: joblib in /Users/Int/Library/Python/2.7/lib/python/site-packages (from nltk) (0.14.1)\n","Collecting regex\n","  Using cached regex-2022.1.18.tar.gz (382 kB)\n","Collecting tqdm\n","  Using cached tqdm-4.64.0-py2.py3-none-any.whl (78 kB)\n","Collecting importlib-resources; python_version < \"3.7\"\n","  Using cached importlib_resources-3.3.1-py2.py3-none-any.whl (26 kB)\n","Requirement already satisfied: pathlib2; python_version < \"3\" in /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages (from importlib-resources; python_version < \"3.7\"->tqdm->nltk) (2.3.5)\n","Requirement already satisfied: contextlib2; python_version < \"3\" in /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages (from importlib-resources; python_version < \"3.7\"->tqdm->nltk) (0.6.0.post1)\n","Collecting typing; python_version < \"3.5\"\n","  Using cached typing-3.10.0.0-py2-none-any.whl (26 kB)\n","Requirement already satisfied: zipp>=0.4; python_version < \"3.8\" in /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages (from importlib-resources; python_version < \"3.7\"->tqdm->nltk) (1.2.0)\n","Requirement already satisfied: singledispatch; python_version < \"3.4\" in /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages (from importlib-resources; python_version < \"3.7\"->tqdm->nltk) (3.4.0.3)\n","Requirement already satisfied: scandir; python_version < \"3.5\" in /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages (from pathlib2; python_version < \"3\"->importlib-resources; python_version < \"3.7\"->tqdm->nltk) (1.10.0)\n","Requirement already satisfied: six in /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages (from pathlib2; python_version < \"3\"->importlib-resources; python_version < \"3.7\"->tqdm->nltk) (1.14.0)\n","Using legacy 'setup.py install' for nltk, since package 'wheel' is not installed.\n","Using legacy 'setup.py install' for regex, since package 'wheel' is not installed.\n","Installing collected packages: regex, typing, importlib-resources, tqdm, nltk\n","    Running setup.py install for regex ... \u001b[?25lerror\n","\u001b[31m    ERROR: Command errored out with exit status 1:\n","     command: /Library/Frameworks/Python.framework/Versions/2.7/Resources/Python.app/Contents/MacOS/Python -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/private/var/folders/4w/kq3dzwvx4qjg1s87_r4kd7bw0000gp/T/pip-install-618Bok/regex/setup.py'\"'\"'; __file__='\"'\"'/private/var/folders/4w/kq3dzwvx4qjg1s87_r4kd7bw0000gp/T/pip-install-618Bok/regex/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record /private/var/folders/4w/kq3dzwvx4qjg1s87_r4kd7bw0000gp/T/pip-record-WJrNRC/install-record.txt --single-version-externally-managed --compile --install-headers /Library/Frameworks/Python.framework/Versions/2.7/include/python2.7/regex\n","         cwd: /private/var/folders/4w/kq3dzwvx4qjg1s87_r4kd7bw0000gp/T/pip-install-618Bok/regex/\n","    Complete output (87 lines):\n","    running install\n","    running build\n","    running build_py\n","    creating build\n","    creating build/lib.macosx-10.9-x86_64-2.7\n","    creating build/lib.macosx-10.9-x86_64-2.7/regex\n","    copying regex_3/__init__.py -> build/lib.macosx-10.9-x86_64-2.7/regex\n","    copying regex_3/regex.py -> build/lib.macosx-10.9-x86_64-2.7/regex\n","    copying regex_3/_regex_core.py -> build/lib.macosx-10.9-x86_64-2.7/regex\n","    copying regex_3/test_regex.py -> build/lib.macosx-10.9-x86_64-2.7/regex\n","    running build_ext\n","    building 'regex._regex' extension\n","    creating build/temp.macosx-10.9-x86_64-2.7\n","    creating build/temp.macosx-10.9-x86_64-2.7/regex_3\n","    gcc -fno-strict-aliasing -fno-common -dynamic -arch x86_64 -g -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -I/Library/Frameworks/Python.framework/Versions/2.7/include/python2.7 -c regex_3/_regex.c -o build/temp.macosx-10.9-x86_64-2.7/regex_3/_regex.o\n","    regex_3/_regex.c:755:23: error: expected expression\n","        return *((Py_UCS1*)text + pos);\n","                          ^\n","    regex_3/_regex.c:755:15: error: use of undeclared identifier 'Py_UCS1'\n","        return *((Py_UCS1*)text + pos);\n","                  ^\n","    regex_3/_regex.c:760:16: error: expected expression\n","        *((Py_UCS1*)text + pos) = (Py_UCS1)ch;\n","                   ^\n","    regex_3/_regex.c:760:8: error: use of undeclared identifier 'Py_UCS1'\n","        *((Py_UCS1*)text + pos) = (Py_UCS1)ch;\n","           ^\n","    regex_3/_regex.c:760:32: error: use of undeclared identifier 'Py_UCS1'\n","        *((Py_UCS1*)text + pos) = (Py_UCS1)ch;\n","                                   ^\n","    regex_3/_regex.c:765:21: error: expected expression\n","        return (Py_UCS1*)text + pos;\n","                        ^\n","    regex_3/_regex.c:765:13: error: use of undeclared identifier 'Py_UCS1'\n","        return (Py_UCS1*)text + pos;\n","                ^\n","    regex_3/_regex.c:770:23: error: expected expression\n","        return *((Py_UCS2*)text + pos);\n","                          ^\n","    regex_3/_regex.c:770:15: error: use of undeclared identifier 'Py_UCS2'\n","        return *((Py_UCS2*)text + pos);\n","                  ^\n","    regex_3/_regex.c:775:16: error: expected expression\n","        *((Py_UCS2*)text + pos) = (Py_UCS2)ch;\n","                   ^\n","    regex_3/_regex.c:775:8: error: use of undeclared identifier 'Py_UCS2'\n","        *((Py_UCS2*)text + pos) = (Py_UCS2)ch;\n","           ^\n","    regex_3/_regex.c:775:32: error: use of undeclared identifier 'Py_UCS2'\n","        *((Py_UCS2*)text + pos) = (Py_UCS2)ch;\n","                                   ^\n","    regex_3/_regex.c:780:21: error: expected expression\n","        return (Py_UCS2*)text + pos;\n","                        ^\n","    regex_3/_regex.c:780:13: error: use of undeclared identifier 'Py_UCS2'\n","        return (Py_UCS2*)text + pos;\n","                ^\n","    regex_3/_regex.c:2103:25: error: use of undeclared identifier 'PyExc_TimeoutError'; did you mean 'PyExc_ImportError'?\n","            PyErr_SetString(PyExc_TimeoutError, \"regex timed out\");\n","                            ^~~~~~~~~~~~~~~~~~\n","                            PyExc_ImportError\n","    /Library/Frameworks/Python.framework/Versions/2.7/include/python2.7/pyerrors.h:134:24: note: 'PyExc_ImportError' declared here\n","    PyAPI_DATA(PyObject *) PyExc_ImportError;\n","                           ^\n","    regex_3/_regex.c:3468:9: error: unknown type name 'Py_UCS1'; did you mean 'Py_UCS4'?\n","            Py_UCS1* text_ptr;\n","            ^~~~~~~\n","            Py_UCS4\n","    /Library/Frameworks/Python.framework/Versions/2.7/include/python2.7/unicodeobject.h:128:22: note: 'Py_UCS4' declared here\n","    typedef unsigned int Py_UCS4;\n","                         ^\n","    regex_3/_regex.c:3469:9: error: unknown type name 'Py_UCS1'; did you mean 'Py_UCS4'?\n","            Py_UCS1* limit_ptr;\n","            ^~~~~~~\n","            Py_UCS4\n","    /Library/Frameworks/Python.framework/Versions/2.7/include/python2.7/unicodeobject.h:128:22: note: 'Py_UCS4' declared here\n","    typedef unsigned int Py_UCS4;\n","                         ^\n","    regex_3/_regex.c:3471:29: error: expected expression\n","            text_ptr = (Py_UCS1*)text + text_pos;\n","                                ^\n","    regex_3/_regex.c:3471:21: error: use of undeclared identifier 'Py_UCS1'\n","            text_ptr = (Py_UCS1*)text + text_pos;\n","                        ^\n","    fatal error: too many errors emitted, stopping now [-ferror-limit=]\n","    20 errors generated.\n","    error: command 'gcc' failed with exit status 1\n","    ----------------------------------------\u001b[0m\n","\u001b[?25h\u001b[31mERROR: Command errored out with exit status 1: /Library/Frameworks/Python.framework/Versions/2.7/Resources/Python.app/Contents/MacOS/Python -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/private/var/folders/4w/kq3dzwvx4qjg1s87_r4kd7bw0000gp/T/pip-install-618Bok/regex/setup.py'\"'\"'; __file__='\"'\"'/private/var/folders/4w/kq3dzwvx4qjg1s87_r4kd7bw0000gp/T/pip-install-618Bok/regex/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record /private/var/folders/4w/kq3dzwvx4qjg1s87_r4kd7bw0000gp/T/pip-record-WJrNRC/install-record.txt --single-version-externally-managed --compile --install-headers /Library/Frameworks/Python.framework/Versions/2.7/include/python2.7/regex Check the logs for full command output.\u001b[0m\n"]},{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package punkt to /Users/Int/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]}],"source":["!pip install pymorphy2 nltk\n","import nltk\n","from nltk.tokenize import word_tokenize\n","import nltk\n","\n","import ssl\n","\n","try:\n","\n","    _create_unverified_https_context = ssl._create_unverified_context\n","\n","except AttributeError:\n","\n","    pass\n","\n","else:\n","\n","    ssl._create_default_https_context = _create_unverified_https_context\n","\n","nltk.download('punkt')\n","from nltk.corpus import stopwords\n","russian_stopwords = stopwords.words(\"russian\")\n"]},{"cell_type":"markdown","metadata":{"id":"NEDnZjcJzHgr"},"source":["Сейчас мы готовы к проведению предобработки текста. Но для того, чтобы работать с моделью word2vec, нам еще понадобится установить библиотеку gensim, в которой описана эта модель."]},{"cell_type":"code","execution_count":28,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4067,"status":"ok","timestamp":1642957518520,"user":{"displayName":"Иван Романов","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"05119643280596747163"},"user_tz":-180},"id":"4FDadZjZLY6R","outputId":"ec320aee-3c90-41cf-f5f1-22b7d07684fa"},"outputs":[{"name":"stdout","output_type":"stream","text":["Defaulting to user installation because normal site-packages is not writeable\n","Requirement already satisfied: gensim in /Users/Int/Library/Python/3.8/lib/python/site-packages (4.1.2)\n","Requirement already satisfied: smart-open>=1.8.1 in /Users/Int/Library/Python/3.8/lib/python/site-packages (from gensim) (5.2.1)\n","Requirement already satisfied: numpy>=1.17.0 in /Users/Int/Library/Python/3.8/lib/python/site-packages (from gensim) (1.22.3)\n","Requirement already satisfied: scipy>=0.18.1 in /Users/Int/Library/Python/3.8/lib/python/site-packages (from gensim) (1.8.0)\n","Note: you may need to restart the kernel to use updated packages.\n"]}],"source":["pip install gensim"]},{"cell_type":"markdown","metadata":{"id":"uKxOHCADM6gL"},"source":["Для демонстрации возможностей этой библиотеки снова возьмем набор данных с текстами твитов, где сразу же переименуем класс -1 в 0. Вспомним, как выглядит наш датасет."]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5823,"status":"ok","timestamp":1642957525327,"user":{"displayName":"Иван Романов","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"05119643280596747163"},"user_tz":-180},"id":"hAIFQoNTgd-2","outputId":"504c9e51-5314-4423-8524-8d832c35a5d2"},"outputs":[],"source":["# from google.colab import drive\n","# import os\n","# # drive.mount('/content/drive/')\n","# drive.mount(\"/content/drive/\", force_remount=True)"]},{"cell_type":"code","execution_count":22,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":455},"executionInfo":{"elapsed":19,"status":"ok","timestamp":1642957525329,"user":{"displayName":"Иван Романов","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"05119643280596747163"},"user_tz":-180},"id":"MrOFcawaLo1s","outputId":"528424e2-3a31-4f4d-f498-cf75e1375e19"},"outputs":[{"name":"stderr","output_type":"stream","text":["/var/folders/4w/kq3dzwvx4qjg1s87_r4kd7bw0000gp/T/ipykernel_13589/1335578510.py:4: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  df.positive[df.positive==-1] = 0\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>date</th>\n","      <th>name</th>\n","      <th>text</th>\n","      <th>positive</th>\n","      <th>rep</th>\n","      <th>rtv</th>\n","      <th>fav</th>\n","      <th>total_count</th>\n","      <th>fol</th>\n","      <th>friends</th>\n","      <th>list_count</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>16</th>\n","      <td>4.089069e+17</td>\n","      <td>1.386326e+09</td>\n","      <td>fantanshik</td>\n","      <td>RT @Abdullin_A_R: @LikhodedovaMary эхх, а в УГ...</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>2074.0</td>\n","      <td>82.0</td>\n","      <td>44.0</td>\n","      <td>5.0</td>\n","    </tr>\n","    <tr>\n","      <th>17</th>\n","      <td>4.089069e+17</td>\n","      <td>1.386326e+09</td>\n","      <td>Tienn_En</td>\n","      <td>@marinaysol а, а то подумала, что у тебя там п...</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>6362.0</td>\n","      <td>30.0</td>\n","      <td>28.0</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>18</th>\n","      <td>4.089069e+17</td>\n","      <td>1.386326e+09</td>\n","      <td>bstrd666</td>\n","      <td>@xLesherx @4EU3 зря вы с этой ерундой шутите))...</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>13431.0</td>\n","      <td>473.0</td>\n","      <td>111.0</td>\n","      <td>2.0</td>\n","    </tr>\n","    <tr>\n","      <th>19</th>\n","      <td>4.089069e+17</td>\n","      <td>1.386326e+09</td>\n","      <td>esken_h</td>\n","      <td>@Moscow_advokat Очень главное спасибо for   МЕ...</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>126795.0</td>\n","      <td>581.0</td>\n","      <td>86.0</td>\n","      <td>10.0</td>\n","    </tr>\n","    <tr>\n","      <th>20</th>\n","      <td>4.089069e+17</td>\n","      <td>1.386326e+09</td>\n","      <td>Obrazcova98</td>\n","      <td>У нас есть прекрасная история, как сдохнуть за...</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>492.0</td>\n","      <td>14.0</td>\n","      <td>23.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>21</th>\n","      <td>4.089068e+17</td>\n","      <td>1.386326e+09</td>\n","      <td>dugarchikbellko</td>\n","      <td>на работе был полный пиддес :| и так каждое за...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>8064.0</td>\n","      <td>111.0</td>\n","      <td>94.0</td>\n","      <td>2.0</td>\n","    </tr>\n","    <tr>\n","      <th>22</th>\n","      <td>4.089068e+17</td>\n","      <td>1.386326e+09</td>\n","      <td>nugemycejela</td>\n","      <td>Коллеги сидят рубятся в Urban terror, а я из-з...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>26.0</td>\n","      <td>42.0</td>\n","      <td>39.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>23</th>\n","      <td>4.089069e+17</td>\n","      <td>1.386326e+09</td>\n","      <td>4post21</td>\n","      <td>@elina_4post как говорят обещаного три года жд...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>718.0</td>\n","      <td>49.0</td>\n","      <td>249.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>24</th>\n","      <td>4.089069e+17</td>\n","      <td>1.386326e+09</td>\n","      <td>Poliwake</td>\n","      <td>Желаю хорошего полёта и удачной посадки,я буду...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>10628.0</td>\n","      <td>207.0</td>\n","      <td>200.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>25</th>\n","      <td>4.089069e+17</td>\n","      <td>1.386326e+09</td>\n","      <td>capyvixowe</td>\n","      <td>Обновил за каким-то лешим surf, теперь не рабо...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>35.0</td>\n","      <td>17.0</td>\n","      <td>34.0</td>\n","      <td>0.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["              id          date             name  \\\n","16  4.089069e+17  1.386326e+09       fantanshik   \n","17  4.089069e+17  1.386326e+09         Tienn_En   \n","18  4.089069e+17  1.386326e+09         bstrd666   \n","19  4.089069e+17  1.386326e+09          esken_h   \n","20  4.089069e+17  1.386326e+09      Obrazcova98   \n","21  4.089068e+17  1.386326e+09  dugarchikbellko   \n","22  4.089068e+17  1.386326e+09     nugemycejela   \n","23  4.089069e+17  1.386326e+09          4post21   \n","24  4.089069e+17  1.386326e+09         Poliwake   \n","25  4.089069e+17  1.386326e+09       capyvixowe   \n","\n","                                                 text  positive  rep  rtv  \\\n","16  RT @Abdullin_A_R: @LikhodedovaMary эхх, а в УГ...       1.0  0.0  1.0   \n","17  @marinaysol а, а то подумала, что у тебя там п...       1.0  0.0  0.0   \n","18  @xLesherx @4EU3 зря вы с этой ерундой шутите))...       1.0  0.0  0.0   \n","19  @Moscow_advokat Очень главное спасибо for   МЕ...       1.0  0.0  0.0   \n","20  У нас есть прекрасная история, как сдохнуть за...       1.0  0.0  0.0   \n","21  на работе был полный пиддес :| и так каждое за...       0.0  0.0  0.0   \n","22  Коллеги сидят рубятся в Urban terror, а я из-з...       0.0  0.0  0.0   \n","23  @elina_4post как говорят обещаного три года жд...       0.0  0.0  0.0   \n","24  Желаю хорошего полёта и удачной посадки,я буду...       0.0  0.0  0.0   \n","25  Обновил за каким-то лешим surf, теперь не рабо...       0.0  0.0  0.0   \n","\n","    fav  total_count    fol  friends  list_count  \n","16  0.0       2074.0   82.0     44.0         5.0  \n","17  0.0       6362.0   30.0     28.0         1.0  \n","18  0.0      13431.0  473.0    111.0         2.0  \n","19  0.0     126795.0  581.0     86.0        10.0  \n","20  0.0        492.0   14.0     23.0         0.0  \n","21  0.0       8064.0  111.0     94.0         2.0  \n","22  0.0         26.0   42.0     39.0         0.0  \n","23  0.0        718.0   49.0    249.0         0.0  \n","24  0.0      10628.0  207.0    200.0         0.0  \n","25  0.0         35.0   17.0     34.0         0.0  "]},"execution_count":22,"metadata":{},"output_type":"execute_result"}],"source":["import pandas as pd\n","\n","df = pd.read_excel('/Users/Int/DS/day6/data-samples/tweets_example.xlsx')\n","df.positive[df.positive==-1] = 0\n","df.loc[16:25]"]},{"cell_type":"markdown","metadata":{"id":"IUBBZHDajqbs"},"source":["Так же, как и при использовании изученных раньше алгоритмов векторизации текста, нам нужно провести очистку данных, т.е. привести все слова текстов к одинаковому виду и форме. Напишем для этого функцию, чтобы можно было потом все эти действия вызывать одной командой."]},{"cell_type":"code","execution_count":29,"metadata":{"id":"vOThpNTu51g8"},"outputs":[],"source":["import re\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","import pymorphy2\n","\n","\n","morph = pymorphy2.MorphAnalyzer()\n","\n","\n","def text_preprocessing(text):\n","    \"\"\"Функция принимает строку и возвращает список слов в начальной форме\"\"\"\n","    text = text.lower()                                                         # приводим текст к нижнему регистру\n","    text = re.sub(r\"[^А-Яа-я]\", \" \", text)                                      # удаляем все некириллические символы\n","    words = word_tokenize(text)                                                 # разбиваем тексты на списки слов\n","    words = [morph.parse(word)[0].normal_form for word in words]                # приводим слова к начальной форме\n","    words = [word for word in words if word not in stopwords.words(\"russian\")]  # удаляем слова из стоп-листа\n","    return words"]},{"cell_type":"code","execution_count":30,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":473},"executionInfo":{"elapsed":10,"status":"ok","timestamp":1642957534292,"user":{"displayName":"Иван Романов","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"05119643280596747163"},"user_tz":-180},"id":"Km1LPs1XiWuw","outputId":"16b36d42-df5c-401f-acc9-96b7d265edf6"},"outputs":[{"name":"stderr","output_type":"stream","text":["/var/folders/4w/kq3dzwvx4qjg1s87_r4kd7bw0000gp/T/ipykernel_13589/2111668481.py:2: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  preprocessed_df.text = df.text.apply(text_preprocessing)\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>text</th>\n","      <th>positive</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>15</th>\n","      <td>[смотреть, случайно, аспирантура, попасть, нау...</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>[эхх, угата, контрольный, предупреждать, контр...</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>17</th>\n","      <td>[подумать, пробежечка]</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>18</th>\n","      <td>[зря, ерунда, шутить, история, наблюдать, режи...</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>19</th>\n","      <td>[очень, главное, спасибо, медведа, работать, к...</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>20</th>\n","      <td>[прекрасный, история, сдохнуть, неделя]</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>21</th>\n","      <td>[работа, полный, пиддес, каждый, закрытие, мес...</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>22</th>\n","      <td>[коллега, сидеть, рубиться, долбать, винд, мочь]</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>23</th>\n","      <td>[говорить, обещаной, год, ждать]</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>24</th>\n","      <td>[желать, хороший, пол, удачный, посадка, очень...</td>\n","      <td>0.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                                 text  positive\n","15  [смотреть, случайно, аспирантура, попасть, нау...       1.0\n","16  [эхх, угата, контрольный, предупреждать, контр...       1.0\n","17                             [подумать, пробежечка]       1.0\n","18  [зря, ерунда, шутить, история, наблюдать, режи...       1.0\n","19  [очень, главное, спасибо, медведа, работать, к...       1.0\n","20            [прекрасный, история, сдохнуть, неделя]       1.0\n","21  [работа, полный, пиддес, каждый, закрытие, мес...       0.0\n","22   [коллега, сидеть, рубиться, долбать, винд, мочь]       0.0\n","23                   [говорить, обещаной, год, ждать]       0.0\n","24  [желать, хороший, пол, удачный, посадка, очень...       0.0"]},"execution_count":30,"metadata":{},"output_type":"execute_result"}],"source":["preprocessed_df = df[[\"text\", \"positive\"]]\n","preprocessed_df.text = df.text.apply(text_preprocessing)\n","preprocessed_df[15:25]"]},{"cell_type":"markdown","metadata":{"id":"pHEm66mM1G5c"},"source":["Обратите внимание, на этот раз мы не удаляем редкие слова, поскольку теперь текст характеризуется не частотностью тех или иных слов, а семантикой слов, входящих в данный текст.\n","\n","Теперь обучим модель word2vec. Поскольку это не предиктивная модель, будем обучать ее на всём множестве текстов, чтобы получить векторное представление как можно большего количества слов. Подробно о параметрах модели можно почитать в официальной [документации](https://radimrehurek.com/gensim/models/word2vec.html)."]},{"cell_type":"code","execution_count":57,"metadata":{"id":"jFA3rDXS1Fkc"},"outputs":[],"source":["from gensim.models import Word2Vec\n","\n","w2v = Word2Vec(vector_size=300, min_count=1)  # создадим экземпляр модели word2vec. Здесь size - размер векторного пространства,\n","                                       # min_count - минимальное количество появлений слова в наборе данных, при котором\n","                                       # будем учитывать это слово в модели\n","w2v.build_vocab(preprocessed_df.text)  # обучим модель на нашем наборе текстов"]},{"cell_type":"markdown","metadata":{"id":"kGs2ZbBZqZ7L"},"source":["Теперь посмотрим, какую информацию о словах мы можем получить из обученной модели. Например, для выбранного нами слова можем посмотреть список наиболее похожих на него слов с точки зрения модели."]},{"cell_type":"code","execution_count":56,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":430,"status":"ok","timestamp":1642957548123,"user":{"displayName":"Иван Романов","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"05119643280596747163"},"user_tz":-180},"id":"O-Wkf1KjlHQ8","outputId":"08a0fb1e-e992-4d95-e94c-81b508c1e813"},"outputs":[{"data":{"text/plain":["[('новый', 0.19879139959812164),\n"," ('вообще', 0.19217325747013092),\n"," ('мальчик', 0.1701221913099289),\n"," ('рубиться', 0.14808665215969086),\n"," ('рада', 0.14677517116069794),\n"," ('обещаной', 0.12424548715353012),\n"," ('винд', 0.11481524258852005),\n"," ('россия', 0.11323853582143784),\n"," ('увидеть', 0.11053305119276047),\n"," ('деревня', 0.10091958194971085)]"]},"execution_count":56,"metadata":{},"output_type":"execute_result"}],"source":["w2v.wv.most_similar(positive=\"ерунда\")  # этот метод возвращает список кортежей, где первый элемент - это слово,\n","                                        # а второй - степень схожести со словом \"ерунда\". Чем ближе это число к 1,\n","                                        # тем ближе по смыслу выведенное слово"]},{"cell_type":"markdown","metadata":{"id":"7zQiScyE2BGl"},"source":["Аналогично можем посмотреть наименее похожие слова:"]},{"cell_type":"code","execution_count":47,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1642936889148,"user":{"displayName":"Иван Романов","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"05119643280596747163"},"user_tz":-180},"id":"8NjTqIMQ1N7x","outputId":"10098551-b4d5-4295-cda1-54a8a6467be0"},"outputs":[{"data":{"text/plain":["[('общество', 0.14702370762825012),\n"," ('страдать', 0.14572249352931976),\n"," ('самый', 0.1357945054769516),\n"," ('задумать', 0.13516545295715332),\n"," ('тп', 0.13276904821395874),\n"," ('зелёный', 0.12756392359733582),\n"," ('полный', 0.11560168117284775),\n"," ('мск', 0.11394201219081879),\n"," ('шаблон', 0.11300007998943329),\n"," ('таки', 0.11047962307929993)]"]},"execution_count":47,"metadata":{},"output_type":"execute_result"}],"source":["w2v.wv.most_similar(negative=[\"ерунда\"])  # аргумент negative говорит о том, что нужно искать наименее похожие слова.\n","                                          # В этом случае числа - это степень непохожести.\n","                                          # Чем ближе к 1, тем слово меньше похоже на \"ерунду\""]},{"cell_type":"markdown","metadata":{"id":"vFYtDT4m6FcN"},"source":["Метод `most_similar` может помочь нам определить слова, которые наиболее похожи на один набор слов и наименее похожи на другой набор слов."]},{"cell_type":"code","execution_count":55,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1642936889149,"user":{"displayName":"Иван Романов","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"05119643280596747163"},"user_tz":-180},"id":"BbrE5uX-6W1F","outputId":"76a97a00-7b90-4507-b61f-56b370e178a3"},"outputs":[{"data":{"text/plain":["[('общество', 0.17890751361846924),\n"," ('гордый', 0.15167193114757538),\n"," ('долго', 0.14699722826480865),\n"," ('добрый', 0.13322556018829346),\n"," ('мочь', 0.1329142153263092),\n"," ('просто', 0.1327962726354599),\n"," ('ждать', 0.12339630722999573),\n"," ('твой', 0.1222352609038353),\n"," ('который', 0.10499397665262222),\n"," ('часть', 0.09969993680715561)]"]},"execution_count":55,"metadata":{},"output_type":"execute_result"}],"source":["w2v.wv.most_similar(positive=[\"подняться\", \"угата\"], negative=[\"погибать\", \"клоун\"])\n","\n","# на этот раз цифры - это некая общая метрика похожести на то, что мы просим"]},{"cell_type":"markdown","metadata":{"id":"5DlsjpnJ5-ML"},"source":["Еще можно посмотреть, насколько похожи два слова из выборки"]},{"cell_type":"code","execution_count":53,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1642936889699,"user":{"displayName":"Иван Романов","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"05119643280596747163"},"user_tz":-180},"id":"uvhQtHCj6xhj","outputId":"bfdaa959-a20f-49e2-b442-bc04be27a216"},"outputs":[{"data":{"text/plain":["0.08130691"]},"execution_count":53,"metadata":{},"output_type":"execute_result"}],"source":["w2v.wv.similarity(\"клоун\", \"работа\")  # ответ может быть отрицательным - это будет означать, что\n","                                         # эта пара слов - больше антонимы, чем синонимы"]},{"cell_type":"markdown","metadata":{"id":"5bMxQ15mx5j0"},"source":["Очевидно, что с теми словами, которых не было в обучающей выборке, модель работать не сможет:"]},{"cell_type":"code","execution_count":50,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":324},"executionInfo":{"elapsed":9,"status":"error","timestamp":1642936890232,"user":{"displayName":"Иван Романов","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"05119643280596747163"},"user_tz":-180},"id":"Ivv3qJpLx4al","outputId":"74553b8f-b4e9-4b55-91d3-2a26073a9823"},"outputs":[{"ename":"KeyError","evalue":"\"Key 'сбербанк' not present\"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","\u001b[1;32m/Users/Int/DS/day6/src/d06_desk.ipynb Cell 22'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/Int/DS/day6/src/d06_desk.ipynb#ch0000021?line=0'>1</a>\u001b[0m w2v\u001b[39m.\u001b[39;49mwv\u001b[39m.\u001b[39;49mmost_similar(\u001b[39m\"\u001b[39;49m\u001b[39mсбербанк\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n","File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/gensim/models/keyedvectors.py:773\u001b[0m, in \u001b[0;36mKeyedVectors.most_similar\u001b[0;34m(self, positive, negative, topn, clip_start, clip_end, restrict_vocab, indexer)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/Int/Library/Python/3.8/lib/python/site-packages/gensim/models/keyedvectors.py?line=770'>771</a>\u001b[0m     mean\u001b[39m.\u001b[39mappend(weight \u001b[39m*\u001b[39m key)\n\u001b[1;32m    <a href='file:///Users/Int/Library/Python/3.8/lib/python/site-packages/gensim/models/keyedvectors.py?line=771'>772</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///Users/Int/Library/Python/3.8/lib/python/site-packages/gensim/models/keyedvectors.py?line=772'>773</a>\u001b[0m     mean\u001b[39m.\u001b[39mappend(weight \u001b[39m*\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_vector(key, norm\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m))\n\u001b[1;32m    <a href='file:///Users/Int/Library/Python/3.8/lib/python/site-packages/gensim/models/keyedvectors.py?line=773'>774</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhas_index_for(key):\n\u001b[1;32m    <a href='file:///Users/Int/Library/Python/3.8/lib/python/site-packages/gensim/models/keyedvectors.py?line=774'>775</a>\u001b[0m         all_keys\u001b[39m.\u001b[39madd(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_index(key))\n","File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/gensim/models/keyedvectors.py:438\u001b[0m, in \u001b[0;36mKeyedVectors.get_vector\u001b[0;34m(self, key, norm)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/Int/Library/Python/3.8/lib/python/site-packages/gensim/models/keyedvectors.py?line=413'>414</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_vector\u001b[39m(\u001b[39mself\u001b[39m, key, norm\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m    <a href='file:///Users/Int/Library/Python/3.8/lib/python/site-packages/gensim/models/keyedvectors.py?line=414'>415</a>\u001b[0m     \u001b[39m\"\"\"Get the key's vector, as a 1D numpy array.\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/Int/Library/Python/3.8/lib/python/site-packages/gensim/models/keyedvectors.py?line=415'>416</a>\u001b[0m \n\u001b[1;32m    <a href='file:///Users/Int/Library/Python/3.8/lib/python/site-packages/gensim/models/keyedvectors.py?line=416'>417</a>\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/Int/Library/Python/3.8/lib/python/site-packages/gensim/models/keyedvectors.py?line=435'>436</a>\u001b[0m \n\u001b[1;32m    <a href='file:///Users/Int/Library/Python/3.8/lib/python/site-packages/gensim/models/keyedvectors.py?line=436'>437</a>\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> <a href='file:///Users/Int/Library/Python/3.8/lib/python/site-packages/gensim/models/keyedvectors.py?line=437'>438</a>\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_index(key)\n\u001b[1;32m    <a href='file:///Users/Int/Library/Python/3.8/lib/python/site-packages/gensim/models/keyedvectors.py?line=438'>439</a>\u001b[0m     \u001b[39mif\u001b[39;00m norm:\n\u001b[1;32m    <a href='file:///Users/Int/Library/Python/3.8/lib/python/site-packages/gensim/models/keyedvectors.py?line=439'>440</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfill_norms()\n","File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/gensim/models/keyedvectors.py:412\u001b[0m, in \u001b[0;36mKeyedVectors.get_index\u001b[0;34m(self, key, default)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/Int/Library/Python/3.8/lib/python/site-packages/gensim/models/keyedvectors.py?line=409'>410</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m default\n\u001b[1;32m    <a href='file:///Users/Int/Library/Python/3.8/lib/python/site-packages/gensim/models/keyedvectors.py?line=410'>411</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///Users/Int/Library/Python/3.8/lib/python/site-packages/gensim/models/keyedvectors.py?line=411'>412</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mKey \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mkey\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m not present\u001b[39m\u001b[39m\"\u001b[39m)\n","\u001b[0;31mKeyError\u001b[0m: \"Key 'сбербанк' not present\""]}],"source":["w2v.wv.most_similar(\"сбербанк\")"]},{"cell_type":"markdown","metadata":{"id":"bmhcJrBO-m-y"},"source":["А теперь давайте посмотрим на график, какие слова как расположены друг относительно друга. По умолчанию модель word2vec отображает все слова в пространство размерности 300. Это означает, что каждое слово превращается в набор из 300 чисел. На мониторе такую размерность отобразить очень сложно, поэтому воспользуемся методом снижения размерности векторного пространства t-SNE. Сожмем наши вектора до размерности 2, чтобы их легко можно было отобразить на плоскости."]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":665},"executionInfo":{"elapsed":2337,"status":"ok","timestamp":1642936924768,"user":{"displayName":"Иван Романов","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"05119643280596747163"},"user_tz":-180},"id":"zGEvziOxEVoX","outputId":"e2264e6d-1c91-4bfe-b0d0-a748461fffbc"},"outputs":[{"ename":"NameError","evalue":"name 'w2v' is not defined","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[1;32m/Users/Int/DS/Python_DS_beginner._Day06-0/src/d06_desk.ipynb Cell 24'\u001b[0m in \u001b[0;36m<cell line: 32>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/Int/DS/Python_DS_beginner._Day06-0/src/d06_desk.ipynb#ch0000023?line=27'>28</a>\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m words_to_show_indices:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/Int/DS/Python_DS_beginner._Day06-0/src/d06_desk.ipynb#ch0000023?line=28'>29</a>\u001b[0m         plt\u001b[39m.\u001b[39mannotate(labels[i], (x[i], y[i]))                        \u001b[39m# для каждого из этих 25 слов отобразим текст на картинке\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/Int/DS/Python_DS_beginner._Day06-0/src/d06_desk.ipynb#ch0000023?line=31'>32</a>\u001b[0m plot_w2v(w2v)\n","\u001b[0;31mNameError\u001b[0m: name 'w2v' is not defined"]}],"source":["from sklearn.manifold import TSNE\n","import numpy as np\n","from matplotlib import pyplot as plt\n","%matplotlib inline\n","import pylab\n","pylab.rcParams['figure.figsize'] = (15, 10)\n","\n","\n","def reduce_dimensions(w2v_model):\n","    \"\"\"Фукнция принимает модель word2vec и возвращает массив абсцисс,\n","    массив ординат и массив слов после снижения размерности\"\"\"\n","    tsne = TSNE(n_components=2, random_state=256)  # создадим экземпляр модели TSNE\n","    vectors = np.asarray(w2v_model.wv.vectors)     # возьмем из модели 300-мерный массив слов-векторов\n","    labels = np.asarray(w2v_model.wv.index_to_key)   # отдельно сохраним соответствие номера вектора и самого слова\n","    vectors = tsne.fit_transform(vectors)          # проведем преобразование каждого вектора в 2-мерный\n","\n","    x = [v[0] for v in vectors]                    # запишем отдельно массив абсцисс и массив ординат\n","    y = [v[1] for v in vectors]\n","    return x, y, labels\n","\n","\n","def plot_w2v(w2v_model):\n","    \"\"\"Функция строит график распределения слов по векторному пространству\n","    размерности 2 исходя из обученной модели word2vec\"\"\"\n","    x, y, labels = reduce_dimensions(w2v_model)                      # получим значения по осям и названия точек (исходные слова)\n","    plt.scatter(x, y)                                                # строим график с точками\n","    words_to_show_indices = np.random.randint(len(labels), size=25)  # выберем 25 случайных слов, которые отобразим на графике\n","    for i in words_to_show_indices:\n","        plt.annotate(labels[i], (x[i], y[i]))                        # для каждого из этих 25 слов отобразим текст на картинке\n","\n","\n","plot_w2v(w2v)                                                        # применим написанные функции к обученной модели"]},{"cell_type":"markdown","metadata":{"id":"RVS2lILcu1CA"},"source":["Кроме вышеперечисленных возможностей, можно обучить модель word2vec на предсказание следующих слов. Обучим ее и попробуем предсказать продолжение твита \"Котёнка вчера носик разбила, плакала и расстраивалась :(\""]},{"cell_type":"code","execution_count":62,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":19674,"status":"ok","timestamp":1642936968050,"user":{"displayName":"Иван Романов","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"05119643280596747163"},"user_tz":-180},"id":"mzpOg4SRvKEa","outputId":"14097e2b-ac12-4c05-89dd-c845ab4beecb"},"outputs":[{"data":{"text/plain":["[('плакать', 0.34837845),\n"," ('нка', 0.26212084),\n"," ('расстраиваться', 0.22428073),\n"," ('кот', 0.14254387),\n"," ('сердце', 0.0015545582),\n"," ('приехать', 0.0013829062),\n"," ('наш', 0.0010170881),\n"," ('котэнить', 0.00073868985),\n"," ('деревня', 0.00071413646),\n"," ('испугаться', 0.0005688586)]"]},"execution_count":62,"metadata":{},"output_type":"execute_result"}],"source":["w2v.train(preprocessed_df.text, total_examples=preprocessed_df.shape[0], epochs=10000)\n","w2v.predict_output_word([\"вчера\", \"носик\", \"разбить\"])"]},{"cell_type":"markdown","metadata":{"id":"eL2mANxvTMve"},"source":["Вообще говоря, списки синонимов и антонимов у нас получились достаточно спорные. Причина этого - малая выборка слов, на которых обучалась модель. Внутри word2vec используется нейросеть-автоэнкодер, а таким моделям всегда нужно много данных для того, чтобы составить корректное отображение входных данных в векторное пространство. Чем больше слов в тексте, тем понятней, какие слова похожи по значению, а какие наоборот противоположны."]},{"cell_type":"markdown","metadata":{"id":"basdbO5LXQHo"},"source":["# Задание 1\n","1. Проведите предобработку текстов из файлов positive.csv, negative.csv. Нужно выполнить те же действия, что в предыдущем дне, но не удалять редко встречающиеся слова. Регулировать использование редких слов будем на уровне модели word2vec. Не забудьте удалить стоп-слова.\n","2. Будем исследовать то, как влияют на качество преобразования *размер целевого векторного пространства* и *использование редких слов*. Создайте несколько моделей word2vec, перебрав параметры:\n","  - размер результирующего пространства: [10, 300, 500] при фиксированной минимальной встречаемости слов = 10\n","  - минимальная встречаемость слов: [1, 10, 100] при фиксированном размере результирующего векторного пространства = 300\n","\n","  Обучите их на всем пространстве текстов.\n","\n","3. Отберите 5 случайных слов из выборки позитивных публикаций и 5 случайных слови из выборки негативных публикаций.\n","4. Для каждой из обученных моделей найдите по 15 синонимов и по 15 антонимов для каждого из слов из п.3. Опишите:\n","  - как влияет размер результирующего пространства на точность определения синонимов/антонимов моделью? почему?\n","  - как влияет минимальная встречаемость слов на точность определения синонимов/антонимов моделью? почему?\n","5. Постройте графики распределения слов в двумерном пространстве. Опишите, как влияют исследуемые параметры на кучность и расположение точек на графике. Почему?\n","6. Возьмите любой твит, обучите модель word2vec с параметрами по умолчанию и попробуйте предсказать продолжение твита. Также попробуйте предсказать продолжение случайной фразы. Сравните результаты, полученные после обучения моделей с разным количеством эпох обучения."]},{"cell_type":"code","execution_count":3,"metadata":{"id":"RbTyuv6Nrn2-"},"outputs":[{"name":"stdout","output_type":"stream","text":["%pylab is deprecated, use %matplotlib inline and import the required libraries.\n","Populating the interactive namespace from numpy and matplotlib\n","[Ok] Все id уникальны.\n","(226834, 12)\n"]},{"ename":"NameError","evalue":"name 'word_tokenize' is not defined","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[1;32m/Users/Int/DS/Python_DS_beginner._Day06-0/src/d06_desk.ipynb Cell 29'\u001b[0m in \u001b[0;36m<cell line: 42>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/Int/DS/Python_DS_beginner._Day06-0/src/d06_desk.ipynb#ch0000028?line=37'>38</a>\u001b[0m df\u001b[39m.\u001b[39mtext \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39mtext\u001b[39m.\u001b[39mstr\u001b[39m.\u001b[39mlower()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/Int/DS/Python_DS_beginner._Day06-0/src/d06_desk.ipynb#ch0000028?line=39'>40</a>\u001b[0m df\u001b[39m.\u001b[39mtext \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39mtext\u001b[39m.\u001b[39mstr\u001b[39m.\u001b[39mreplace(\u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m[^А-Яа-я]\u001b[39m\u001b[39m\"\u001b[39m,\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/Int/DS/Python_DS_beginner._Day06-0/src/d06_desk.ipynb#ch0000028?line=41'>42</a>\u001b[0m df\u001b[39m.\u001b[39mtext \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mmap\u001b[39m(word_tokenize, df\u001b[39m.\u001b[39mtext))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/Int/DS/Python_DS_beginner._Day06-0/src/d06_desk.ipynb#ch0000028?line=42'>43</a>\u001b[0m df\u001b[39m.\u001b[39mtext\u001b[39m.\u001b[39mloc[\u001b[39m19\u001b[39m:\u001b[39m22\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/Int/DS/Python_DS_beginner._Day06-0/src/d06_desk.ipynb#ch0000028?line=43'>44</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnltk\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcorpus\u001b[39;00m \u001b[39mimport\u001b[39;00m stopwords\n","\u001b[0;31mNameError\u001b[0m: name 'word_tokenize' is not defined"]}],"source":["# код выполнения задания 1\n","import pandas as pd\n","import numpy as np\n","import warnings\n","warnings.filterwarnings('ignore')\n","from IPython.core.display import Image, display\n","%matplotlib inline\n","import seaborn as sns\n","from matplotlib import pyplot as plt\n","import pylab as pl\n","import re\n","import codecs\n","import nltk\n","import pymorphy2\n","from sklearn.metrics import classification_report\n","from sklearn.linear_model import LogisticRegression\n","\n","%pylab inline\n","pylab.rcParams['figure.figsize'] = (15,10)\n","\n","df = pd.read_excel('/Users/Int/DS/day6/data-samples/tweets_example.xlsx')\n","df.loc[16:25]\n","columns = df.columns\n","pos = pd.read_csv('/Users/Int/DS/day6/datasets/positive.csv', sep=';', names=columns, error_bad_lines=False)\n","neg = pd.read_csv('/Users/Int/DS/day6/datasets/negative.csv',  sep=';', names=columns, error_bad_lines=False)\n","\n","df = pd.concat([pos, neg],ignore_index=True)\n","\n","if (df['id'].is_unique):\n","\tprint(\"[Ok] Все id уникальны.\")\n","else:\n","\tprint(\"[Ошибка!] Повторяющиеся id!\")\n","print(df.shape)\n","\n","\n","df.index.is_monotonic_increasing\n","\n","df.text = df.text.str.lower()\n","\n","df.text = df.text.str.replace(r\"[^А-Яа-я]\",\" \")\n","\n","df.text = list(map(word_tokenize, df.text))\n","df.text.loc[19:22]\n","from nltk.corpus import stopwords\n","russian_stopwords = stopwords.words(\"russian\")\n","russian_stopwords.sort()\n","russian_stopwords\n","\n","def delete_stopword(words):\n","    global russian_stopwords\n","    new_s = [word for word in words if word not in russian_stopwords]\n","    return new_s\n","\n","df.text = list(map(delete_stopword, df.text))\n","preprocessed_df = df[[\"text\", \"positive\"]]\n","preprocessed_df\n","# preprocessed_df = df\n","\n","from gensim.models import Word2Vec\n","#2\n","# w2v10 = Word2Vec(vector_size=10, min_count=10)\n","# w2v300 = Word2Vec(vector_size=300, min_count=10)\n","# w2v500 = Word2Vec(vector_size=500, min_count=10)\n","\n","w2v1 = Word2Vec(vector_size=300, min_count=1)  # создадим экземпляр модели word2vec. Здесь size - размер векторного пространства,\n","                                       # min_count - минимальное количество появлений слова в наборе данных, при котором\n","                                      # будем учитывать это слово в модели\n","# w2v10 = Word2Vec(vector_size=300, min_count=10)\n","# w2v100 = Word2Vec(vector_size=300, min_count=100)\n","\n","w2v.build_vocab(preprocessed_df.text)\n","\n","from sklearn.manifold import TSNE\n","import numpy as np\n","from matplotlib import pyplot as plt\n","%matplotlib inline\n","import pylab\n","pylab.rcParams['figure.figsize'] = (15, 10)\n","\n","\n","def reduce_dimensions(w2v_model):\n","    \"\"\"Фукнция принимает модель word2vec и возвращает массив абсцисс,\n","    массив ординат и массив слов после снижения размерности\"\"\"\n","    tsne = TSNE(n_components=2, random_state=256)  # создадим экземпляр модели TSNE\n","    vectors = np.asarray(w2v_model.wv.vectors)     # возьмем из модели 300-мерный массив слов-векторов\n","    labels = np.asarray(w2v_model.wv.index_to_key)   # отдельно сохраним соответствие номера вектора и самого слова\n","    vectors = tsne.fit_transform(vectors)          # проведем преобразование каждого вектора в 2-мерный\n","\n","    x = [v[0] for v in vectors]                    # запишем отдельно массив абсцисс и массив ординат\n","    y = [v[1] for v in vectors]\n","    return x, y, labels\n","\n","\n","def plot_w2v(w2v_model):\n","    \"\"\"Функция строит график распределения слов по векторному пространству\n","    размерности 2 исходя из обученной модели word2vec\"\"\"\n","    x, y, labels = reduce_dimensions(w2v_model)                      # получим значения по осям и названия точек (исходные слова)\n","    plt.scatter(x, y)                                                # строим график с точками\n","    words_to_show_indices = np.random.randint(len(labels), size=25)  # выберем 25 случайных слов, которые отобразим на графике\n","    for i in words_to_show_indices:\n","        plt.annotate(labels[i], (x[i], y[i]))                        # для каждого из этих 25 слов отобразим текст на картинке\n","\n","\n","plot_w2v(w2v) \n","#3\n","# words_to_show_pos = np.random.randint(df.positive[df.positive==-1] , size=5) \n","# words_to_show_neg = np.random.randint(df.positive[df.positive==1] , size=5) \n","# words_to_show_pos\n","# words_to_show_neg\n","# w2v.train(preprocessed_df.text, total_examples=preprocessed_df.shape[0], epochs=10000)\n","# w2v.predict_output_word([\"вчера\", \"носик\", \"разбить\"])"]},{"cell_type":"code","execution_count":99,"metadata":{},"outputs":[{"ename":"AttributeError","evalue":"'Series' object has no attribute 'text'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[1;32m/Users/Int/DS/day6/src/d06_desk.ipynb Cell 30'\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/Int/DS/day6/src/d06_desk.ipynb#ch0000040?line=11'>12</a>\u001b[0m \u001b[39m# Выберем пять негативных слов\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/Int/DS/day6/src/d06_desk.ipynb#ch0000040?line=12'>13</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m0\u001b[39m, \u001b[39m5\u001b[39m):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/Int/DS/day6/src/d06_desk.ipynb#ch0000040?line=13'>14</a>\u001b[0m   str_p \u001b[39m=\u001b[39m random\u001b[39m.\u001b[39mchoice(\u001b[39mlist\u001b[39m(preprocessed_df[df\u001b[39m.\u001b[39;49mpositive \u001b[39m==\u001b[39;49m \u001b[39m0\u001b[39;49m]\u001b[39m.\u001b[39;49mtext))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/Int/DS/day6/src/d06_desk.ipynb#ch0000040?line=14'>15</a>\u001b[0m   word \u001b[39m=\u001b[39m random\u001b[39m.\u001b[39msample(str_p, \u001b[39m1\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/Int/DS/day6/src/d06_desk.ipynb#ch0000040?line=15'>16</a>\u001b[0m   \u001b[39mset\u001b[39m\u001b[39m.\u001b[39mappend(word)\n","File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/pandas/core/generic.py:5583\u001b[0m, in \u001b[0;36mNDFrame.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   <a href='file:///Users/Int/Library/Python/3.8/lib/python/site-packages/pandas/core/generic.py?line=5575'>5576</a>\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m   <a href='file:///Users/Int/Library/Python/3.8/lib/python/site-packages/pandas/core/generic.py?line=5576'>5577</a>\u001b[0m     name \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_internal_names_set\n\u001b[1;32m   <a href='file:///Users/Int/Library/Python/3.8/lib/python/site-packages/pandas/core/generic.py?line=5577'>5578</a>\u001b[0m     \u001b[39mand\u001b[39;00m name \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_metadata\n\u001b[1;32m   <a href='file:///Users/Int/Library/Python/3.8/lib/python/site-packages/pandas/core/generic.py?line=5578'>5579</a>\u001b[0m     \u001b[39mand\u001b[39;00m name \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_accessors\n\u001b[1;32m   <a href='file:///Users/Int/Library/Python/3.8/lib/python/site-packages/pandas/core/generic.py?line=5579'>5580</a>\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_info_axis\u001b[39m.\u001b[39m_can_hold_identifiers_and_holds_name(name)\n\u001b[1;32m   <a href='file:///Users/Int/Library/Python/3.8/lib/python/site-packages/pandas/core/generic.py?line=5580'>5581</a>\u001b[0m ):\n\u001b[1;32m   <a href='file:///Users/Int/Library/Python/3.8/lib/python/site-packages/pandas/core/generic.py?line=5581'>5582</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m[name]\n\u001b[0;32m-> <a href='file:///Users/Int/Library/Python/3.8/lib/python/site-packages/pandas/core/generic.py?line=5582'>5583</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mobject\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m__getattribute__\u001b[39;49m(\u001b[39mself\u001b[39;49m, name)\n","\u001b[0;31mAttributeError\u001b[0m: 'Series' object has no attribute 'text'"]}],"source":["# 4 Выберем случайные слова из датафрейма\n","import random\n","\n","set = []\n","preprocessed_df = preprocessed_df[df.positive == 1].text\n","# Выберем пять позитивных слов\n","for i in range(0, 5):\n","  str_p = random.choice(list(preprocessed_df))\n","  word = random.sample(str_p, 1)\n","  set.append(word)\n","\n","# Выберем пять негативных слов\n","for i in range(0, 5):\n","  str_p = random.choice(list(preprocessed_df[df.positive == 0].text))\n","  word = random.sample(str_p, 1)\n","  set.append(word)\n","\n","set"]},{"cell_type":"markdown","metadata":{"id":"OpZspJounApt"},"source":["# 2. Использование градиентного бустинга над решающими деревьями для решения задачи классификации текстов\n","\n","В предыдущем разделе мы научились превращать слова в векторы. Но перед нами стоит задача классификации текста, а не одного слова, поэтому нам нужно придумать способ, как целый текст представить числами.\n","\n","После преобразования отдельные слова стали векторами, значения которых зависят от семантики слова. Будем рассматривать твит как сущность с усредненной семантикой всех содержащихся в нём слов. Таким образом, для преобразования целого текста в вектор, нам нужно получить средний вектор всех содержащихся в нём слов. Такой способ реализован в модели Doc2Vec в библиотеке gensim."]},{"cell_type":"code","execution_count":87,"metadata":{"id":"WayTnYoIE_Ll"},"outputs":[],"source":["from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n","\n","tweets = [TaggedDocument(doc, [i]) for i, doc in enumerate(preprocessed_df.text)]  # преобразуем наши тексты в объекты, понятные док-2-веку\n","d2v = Doc2Vec(tweets, min_count=2)                        # создадим модель Doc2Vec\n","d2v.train(tweets, total_examples=len(tweets), epochs=20)  # подберем веса коэффициентов внутри модели, которые больше будут подходить к нашему набору текстов"]},{"cell_type":"code","execution_count":88,"metadata":{"id":"IJq2h1nKtCkL"},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","\n","# разобьем набор текстов на тренировочную и тестовую выборки\n","X_train_texts, X_test_texts, y_train, y_test = train_test_split(preprocessed_df.text, preprocessed_df.positive, test_size=0.2, random_state=21)"]},{"cell_type":"code","execution_count":90,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2,"status":"ok","timestamp":1642960846348,"user":{"displayName":"Иван Романов","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"05119643280596747163"},"user_tz":-180},"id":"9ChvJJgHDlRT","outputId":"a2d233ab-89f9-49d1-84fd-fc6292d7567b"},"outputs":[{"data":{"text/plain":["114468    [спасибо, григорчуку, победу, черноморец, прос...\n","115015    [проверяли, сеня, больнички, хорошего, чувству...\n","147222          [тухнем, истории, павлуха, придет, сказала]\n","119060    [моему, ходили, водкой, часа, этим, обработать...\n","74753                                    [похоже, заметили]\n","                                ...                        \n","81968     [добавил, фотографий, альбом, дело, гуляли, св...\n","140036    [всем, буду, писать, пиздец, происходит, недел...\n","202552    [ооо, чувак, понимаю, жил, однокомнатной, квар...\n","70863     [родители, защищают, свое, говно, ахахаха, кла...\n","80841     [мелким, коллекция, фишек, всеми, покемонаме, ...\n","Name: text, Length: 181467, dtype: object"]},"execution_count":90,"metadata":{},"output_type":"execute_result"}],"source":["X_train_texts  # пока что наши тексты выглядят как списки слов в начальной форме, но нам нужно получить из этого векторы"]},{"cell_type":"code","execution_count":91,"metadata":{"id":"JtHXANZiH3-8"},"outputs":[],"source":["def transform_text_array_to_vector_dataframe(text_array):\n","    \"\"\"Функция, которая преобразует одномерный колонку списков слов из текстов\n","    в датафрейм со значениями векторов этих текстов\"\"\"\n","    columns = [str(n) for n in range(d2v.vector_size)]               # задаем список названий колонок - просто порядковые номера\n","    vectors_ndarray = text_array.apply(d2v.infer_vector).to_list()  # прогоняем каждый текст через модель doc2vec и формируем многомерный массив чисел\n","    return pd.DataFrame(vectors_ndarray, columns=columns)            # оборачиваем его в датафрейм для удобства\n","\n","\n","X_train = transform_text_array_to_vector_dataframe(X_train_texts)    # наконец создадим датафреймы, которые сможем подать в модель классификации\n","X_test = transform_text_array_to_vector_dataframe(X_test_texts)"]},{"cell_type":"markdown","metadata":{"id":"nzBBKDL6Gjtw"},"source":["Вы уже познакомились с некоторыми \"деревянными\" методами машинного обучения - решающим деревом и случайным лесом. Градиентный бустинг - это итеративный способ построения классификации, полученный (как и случайный лес) путем комбинации нескольких алгоритмов. Сначала строится обычное решающее дерево. Затем строится ряд моделей, предсказывающих ошибку исходной модели. Эти предсказания вычитаются из исходной модели. Таким образом, в итоге мы имеем один классификатор, но намного более точный, чем обычное решающее дерево. Чем больше итераций этого алгоритма будет проведено, тем выше получится качество модели, но она будет дольше обучаться.\n","\n","Градиентный бустинг над решающими деревьями - это, пожалуй, самая распространенная на сегодняшний день модель машинного обучения. Ее используют для решениях многих задач, начиная кредитным скорингом и заканчивая товарным спросом и антифродом. Мы будем использовать эту модель для классификации наших текстов.\n","\n","Модель градиентного бустинга есть в библиотеке sklearn, но на больших данных она будет обучаться долго. На рынке сейчас популярны несколько оптимизированных реализаций градиентного бустинга. Самые известные - xgboost, lightgbm и catboost. Рассмотрим xgboost, но вы можете использовать любую из этих трех."]},{"cell_type":"code","execution_count":72,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3684,"status":"ok","timestamp":1642961969186,"user":{"displayName":"Иван Романов","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"05119643280596747163"},"user_tz":-180},"id":"3LZxnGLQGEUy","outputId":"2aa50247-fb49-43a2-a7da-5b0cac08efe2"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[33mDEPRECATION: Python 2.7 reached the end of its life on January 1st, 2020. Please upgrade your Python as Python 2.7 is no longer maintained. pip 21.0 will drop support for Python 2.7 in January 2021. More details about Python 2 support in pip can be found at https://pip.pypa.io/en/latest/development/release-process/#python-2-support pip 21.0 will remove support for this functionality.\u001b[0m\n","Collecting xgboost\n","  Downloading xgboost-0.82.tar.gz (665 kB)\n","\u001b[K     |████████████████████████████████| 665 kB 1.8 MB/s eta 0:00:01\n","\u001b[?25h^C\n","\u001b[31mERROR: Operation cancelled by user\u001b[0m\n"]}],"source":["# установим библиотеку\n","!pip install xgboost"]},{"cell_type":"markdown","metadata":{"id":"wwY04bh6PP9G"},"source":["Работать с моделью xgboost можно так же, как с моделями sklearn: fit и predict. Основные гиперпараметры - максимальная глубина деревьев модели и количество деревьев."]},{"cell_type":"code","execution_count":92,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":415,"status":"ok","timestamp":1642963177519,"user":{"displayName":"Иван Романов","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"05119643280596747163"},"user_tz":-180},"id":"9nTvmczdKg95","outputId":"c86bf163-d215-435e-e83a-3650b4220c24"},"outputs":[{"name":"stdout","output_type":"stream","text":["[19:57:43] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n","              precision    recall  f1-score   support\n","\n","          -1       0.61      0.61      0.61     22300\n","           1       0.63      0.62      0.62     23067\n","\n","    accuracy                           0.62     45367\n","   macro avg       0.62      0.62      0.62     45367\n","weighted avg       0.62      0.62      0.62     45367\n","\n"]}],"source":["from xgboost import XGBClassifier\n","from sklearn.metrics import classification_report\n","\n","xgb = XGBClassifier(max_depth=10, n_estimators=50)\n","xgb.fit(X_train, y_train)\n","y_pred = xgb.predict(X_test)\n","print(classification_report(y_pred, y_test))"]},{"cell_type":"markdown","metadata":{"id":"uZcI-pQiPuPp"},"source":["# Задание 2\n","\n","В этом задании от вас требуется провести классификацию текстов с использованием градиентного бустинга. Постройте такую модель, которая даст наилучший результат по метрике precision к классу 0, подобрав гиперпараметры:\n","- минимальная встречаемость слова в текстах в doc2vec\n","- максимальная глубина деревеьев в бустинге\n","- количество деревьев в бустинге\n","\n","Дайте ответ на вопрос: лучше использовать более глубокие или более мелкие деревья в модели градиентного бустинга?"]},{"cell_type":"code","execution_count":94,"metadata":{"id":"wLsUThFwPAtZ"},"outputs":[{"name":"stdout","output_type":"stream","text":["[22:18:20] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n","              precision    recall  f1-score   support\n","\n","          -1       0.59      0.59      0.59     22431\n","           1       0.60      0.60      0.60     22936\n","\n","    accuracy                           0.60     45367\n","   macro avg       0.60      0.60      0.60     45367\n","weighted avg       0.60      0.60      0.60     45367\n","\n","[22:19:16] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n","              precision    recall  f1-score   support\n","\n","          -1       0.61      0.61      0.61     22284\n","           1       0.62      0.62      0.62     23083\n","\n","    accuracy                           0.62     45367\n","   macro avg       0.62      0.62      0.62     45367\n","weighted avg       0.62      0.62      0.62     45367\n","\n","[22:24:08] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n","              precision    recall  f1-score   support\n","\n","          -1       0.60      0.62      0.61     21848\n","           1       0.64      0.62      0.63     23519\n","\n","    accuracy                           0.62     45367\n","   macro avg       0.62      0.62      0.62     45367\n","weighted avg       0.62      0.62      0.62     45367\n","\n","[23:07:49] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n","              precision    recall  f1-score   support\n","\n","          -1       0.60      0.59      0.59     22699\n","           1       0.60      0.60      0.60     22668\n","\n","    accuracy                           0.60     45367\n","   macro avg       0.60      0.60      0.60     45367\n","weighted avg       0.60      0.60      0.60     45367\n","\n","[23:08:42] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n","              precision    recall  f1-score   support\n","\n","          -1       0.60      0.61      0.61     22019\n","           1       0.63      0.62      0.62     23348\n","\n","    accuracy                           0.62     45367\n","   macro avg       0.62      0.62      0.62     45367\n","weighted avg       0.62      0.62      0.62     45367\n","\n"]}],"source":["# код выполнения задания 2\n","\n","from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n","\n","tweets = [TaggedDocument(doc, [i]) for i, doc in enumerate(preprocessed_df.text)]  # преобразуем наши тексты в объекты, понятные док-2-веку\n","d2v = Doc2Vec(tweets, min_count=2)                        # создадим модель Doc2Vec\n","d2v.train(tweets, total_examples=len(tweets), epochs=20)  # подберем веса ко\n","\n","from sklearn.model_selection import train_test_split\n","\n","# разобьем набор текстов на тренировочную и тестовую выборки\n","X_train_texts, X_test_texts, y_train, y_test = train_test_split(preprocessed_df.text, preprocessed_df.positive, test_size=0.2, random_state=21)\n","X_train_texts \n","def transform_text_array_to_vector_dataframe(text_array):\n","    \"\"\"Функция, которая преобразует одномерный колонку списков слов из текстов\n","    в датафрейм со значениями векторов этих текстов\"\"\"\n","    columns = [str(n) for n in range(d2v.vector_size)]               # задаем список названий колонок - просто порядковые номера\n","    vectors_ndarray = text_array.apply(d2v.infer_vector).to_list()  # прогоняем каждый текст через модель doc2vec и формируем многомерный массив чисел\n","    return pd.DataFrame(vectors_ndarray, columns=columns)            # оборачиваем его в датафрейм для удобства\n","\n","\n","X_train = transform_text_array_to_vector_dataframe(X_train_texts)    # наконец создадим датафреймы, которые сможем подать в модель классификации\n","X_test = transform_text_array_to_vector_dataframe(X_test_texts)\n","\n","# !pip install xgboost\n","\n","from xgboost import XGBClassifier\n","from sklearn.metrics import classification_report\n","\n","xgb = XGBClassifier(max_depth=2, n_estimators=50)\n","xgb.fit(X_train, y_train)\n","y_pred = xgb.predict(X_test)\n","print(classification_report(y_pred, y_test))\n","\n","xgb = XGBClassifier(max_depth=10, n_estimators=50)\n","xgb.fit(X_train, y_train)\n","y_pred = xgb.predict(X_test)\n","print(classification_report(y_pred, y_test))\n","\n","xgb = XGBClassifier(max_depth=100, n_estimators=50)\n","xgb.fit(X_train, y_train)\n","y_pred = xgb.predict(X_test)\n","print(classification_report(y_pred, y_test))\n","\n","xgb = XGBClassifier(max_depth=10, n_estimators=10)\n","xgb.fit(X_train, y_train)\n","y_pred = xgb.predict(X_test)\n","print(classification_report(y_pred, y_test))\n","\n","xgb = XGBClassifier(max_depth=10, n_estimators=100)\n","xgb.fit(X_train, y_train)\n","y_pred = xgb.predict(X_test)\n","print(classification_report(y_pred, y_test))"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyMf1F/JoqBtk2YeZbPQdWec","collapsed_sections":[],"name":"d07_task.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.9"}},"nbformat":4,"nbformat_minor":0}
